{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7770020,"sourceType":"datasetVersion","datasetId":4545499}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">1. Introdução</span>\nNeste notebook, o desafio será identificar e localizar gatos em imagens usando a arquitetura de detecção de objetos YOLOv8. Serão realizadas as etapas de pré-processamento dos dados, treinamento do modelo, validação e inferência, culminando na detecção eficaz de gatos nas imagens fornecidas.\n\nA escolha do modelo **YOLOv8** é adequada por vários motivos:\n\n**Alta Precisão**: O desafio envolve a detecção de objetos em imagens, para o qual o YOLOv8 é otimizado, oferecendo reconhecimento confiável, essencial para processar o conjunto de dados com eficiência.\n\n**Facilidade de Uso e Integração**: O modelo vem com uma API simples e exemplos de código, facilitando o desenvolvimento e a integração em soluções existentes.\n\n**Desempenho em Diferentes Condições**: O YOLOv8 é projetado para funcionar bem em uma variedade de condições de imagem, o que é relevante dado que o dataset inclui diversas representações de gatos, sozinhos ou com outros animais.\n\n**Recursos e Suporte da Comunidade**: Há amplo suporte comunitário e documentação disponível para o YOLOv8.\n\nEssas características alinham-se às necessidades do desafio de criar um modelo robusto e eficiente para detecção de gatos em imagens variadas.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">2. Configuração do Ambiente\nAntes da análise e processamento de dados, é essencial configurar nosso ambiente instalando todas as bibliotecas necessárias. Este passo garante que temos todas as ferramentas disponíveis.","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics\n!pip install -U ipywidgets","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:00:21.850298Z","iopub.execute_input":"2024-03-07T12:00:21.851065Z","iopub.status.idle":"2024-03-07T12:00:51.869755Z","shell.execute_reply.started":"2024-03-07T12:00:21.851029Z","shell.execute_reply":"2024-03-07T12:00:51.868607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom ultralytics import YOLO\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport shutil\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:00:51.872384Z","iopub.execute_input":"2024-03-07T12:00:51.873226Z","iopub.status.idle":"2024-03-07T12:01:00.505360Z","shell.execute_reply.started":"2024-03-07T12:00:51.873185Z","shell.execute_reply":"2024-03-07T12:01:00.504594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As bibliotecas essenciais para nosso projeto, incluindo `ultralytics` para YOLO e `sklearn` para divisão do conjunto de dados, foram instaladas com sucesso. Agora pode-se começar a analisar e processar os dados.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">3. Análise Exploratória dos Dados\n## <span style=\"color:#4d4d4d\">3.1 Contagem de Imagens no Dataset\n    Iniciaremos nossa análise exploratória contando o número de imagens disponíveis para treinamento e inferência. Isso nos dará uma visão geral do tamanho do nosso conjunto de dados.\n","metadata":{}},{"cell_type":"code","source":"imgs_path = '/kaggle/input/cats-dataset/cats_dataset/imgs'\ninference_imgs_path = '/kaggle/input/cats-dataset/cats_dataset/inference_imgs'\n\nimgs_files = [file for file in os.listdir(imgs_path) if file.endswith('.jpg') or file.endswith('.png')]\ninference_imgs_files = [file for file in os.listdir(inference_imgs_path) if file.endswith('.JPG')]\n\nnum_imgs = len(imgs_files)\nnum_inference_imgs = len (inference_imgs_files)\nprint(f\"Total de imagens disponíveis para treinamento: {num_imgs}\")\nprint(f\"Total de imagens disponíveis para inferência: {num_inference_imgs}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:00.506602Z","iopub.execute_input":"2024-03-07T12:01:00.507430Z","iopub.status.idle":"2024-03-07T12:01:00.596372Z","shell.execute_reply.started":"2024-03-07T12:01:00.507391Z","shell.execute_reply":"2024-03-07T12:01:00.595438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    O resultado indica que temos um total de 241 imagens para treinamento e 2 para inferência, o que nos fornece uma base sólida para construir nosso modelo de detecção de gatos.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#4d4d4d\">3.2 Correspondência entre Imagens e Anotações\n    Agora, vamos verificar a correspondência entre imagens e anotações. É crucial que cada imagem tenha um arquivo de anotação correspondente para que o treinamento do modelo possa ser executado corretamente.","metadata":{}},{"cell_type":"code","source":"labels_path = '/kaggle/input/cats-dataset/cats_dataset/labels'\n\nlabel_files = [file for file in os.listdir(labels_path) if file.endswith('.txt')]\n\nnum_labels = len(label_files)\nprint(f\"Total de anotações disponíveis: {num_labels}\")\n\nmissing_labels = [img for img in imgs_files if img.replace('.jpg', '.txt').replace('.png', '.txt') not in label_files]\nmissing_imgs = [label for label in label_files if label.replace('.txt', '.jpg').replace('.txt', '.png') not in imgs_files]\n\nprint(f\"Imagens sem anotação correspondente: {len(missing_labels)}\")\nprint(f\"Anotações sem imagem correspondente: {len(missing_imgs)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:00.598840Z","iopub.execute_input":"2024-03-07T12:01:00.599129Z","iopub.status.idle":"2024-03-07T12:01:00.679623Z","shell.execute_reply.started":"2024-03-07T12:01:00.599103Z","shell.execute_reply":"2024-03-07T12:01:00.678780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Todas as imagens têm suas anotações correspondentes, e não há anotações sem imagens correspondentes. Isso confirma que nosso conjunto de dados está completo e pronto para a próxima etapa.\n","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#4d4d4d\">3.3 Análise dos Arquivos de Anotação\n    A seguir, analisaremos o conteúdo dos arquivos de anotação para garantir que estejam formatados corretamente. Isso envolve ler e exibir as coordenadas das bounding boxes.\n","metadata":{}},{"cell_type":"code","source":"example_label_file = label_files[0]\nwith open(os.path.join(labels_path, example_label_file), 'r') as file:\n    example_labels = file.readlines()\n\nprint(f\"Exemplo de anotações do arquivo {example_label_file}:\\n\", example_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:00.680846Z","iopub.execute_input":"2024-03-07T12:01:00.681220Z","iopub.status.idle":"2024-03-07T12:01:00.691814Z","shell.execute_reply.started":"2024-03-07T12:01:00.681184Z","shell.execute_reply":"2024-03-07T12:01:00.690742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    As anotações do arquivo mostram as coordenadas das bounding boxes em formato normalizado, o que é compatível com a entrada necessária para o modelo YOLOv8.\n","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#4d4d4d\">3.4 Visualização das Anotações\n    Para garantir que as anotações estão corretas, visualizaremos as bounding boxes sobrepostas em algumas imagens de exemplo. Isso nos ajuda a confirmar visualmente a precisão das anotações.","metadata":{}},{"cell_type":"code","source":"def draw_bounding_boxes(image_path, annotation_lines):\n    with Image.open(image_path) as im:\n        draw = ImageDraw.Draw(im)\n        for annotation in annotation_lines:\n            class_id, x_center, y_center, width, height = map(float, annotation.split())\n            x1 = (x_center - width / 2) * im.width\n            y1 = (y_center - height / 2) * im.height\n            x2 = (x_center + width / 2) * im.width\n            y2 = (y_center + height / 2) * im.height\n            draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\", width=2)\n        return im\n\nsample_image_path = os.path.join(imgs_path, imgs_files[130])\nsample_label_path = os.path.join(labels_path, imgs_files[130].replace('.jpg', '.txt'))\n\nwith open(sample_label_path, 'r') as file:\n    sample_annotations = file.readlines()\n\ndrawn_image = draw_bounding_boxes(sample_image_path, sample_annotations)\ndrawn_image\n","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:00.693135Z","iopub.execute_input":"2024-03-07T12:01:00.693399Z","iopub.status.idle":"2024-03-07T12:01:00.786971Z","shell.execute_reply.started":"2024-03-07T12:01:00.693376Z","shell.execute_reply":"2024-03-07T12:01:00.786084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    As bounding boxes foram desenhadas corretamente sobre a imagem, indicando que nossas anotações estão no formato correto e que o modelo poderá interpretá-las apropriadamente durante o treinamento.\n","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">4. Pré-processamento dos Dados\n\n## <span style=\"color:#4d4d4d\">4.1 Divisão dos Dados\n\n    A correta divisão do dataset é essencial para um treinamento eficaz e uma validação justa do modelo. Aqui dividimos o conjunto de imagens em três partes distintas: treinamento (80%), validação (10%) e teste (10%). Esta organização dos dados nos ajudará a minimizar o sobreajuste e a verificar o desempenho do modelo em dados não vistos.","metadata":{}},{"cell_type":"code","source":"files = [os.path.splitext(file)[0] for file in os.listdir(imgs_path) if file.endswith('.jpg')]\n\ntrain_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\nval_files, test_files = train_test_split(test_files, test_size=0.5, random_state=42)\n\ndef prepare_data(files, dataset_type, imgs_path, labels_path):\n    os.makedirs(f'/kaggle/working/{dataset_type}/images', exist_ok=True)\n    os.makedirs(f'/kaggle/working/{dataset_type}/labels', exist_ok=True)\n    for file in files:\n        img_src = os.path.join(imgs_path, file + '.jpg')\n        label_src = os.path.join(labels_path, file + '.txt')\n        shutil.copy(img_src, f'/kaggle/working/{dataset_type}/images/')\n        shutil.copy(label_src, f'/kaggle/working/{dataset_type}/labels/')\n\nprepare_data(train_files, 'train', imgs_path, labels_path)\nprepare_data(val_files, 'val', imgs_path, labels_path)\nprepare_data(test_files, 'test', imgs_path, labels_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:00.788066Z","iopub.execute_input":"2024-03-07T12:01:00.788397Z","iopub.status.idle":"2024-03-07T12:01:03.623732Z","shell.execute_reply.started":"2024-03-07T12:01:00.788367Z","shell.execute_reply":"2024-03-07T12:01:03.622877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    Os dados foram divididos com sucesso nas proporções definidas. Os conjuntos de treinamento, validação e teste estão prontos, com as imagens e anotações alocadas para cada fase do processo de modelagem.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#4d4d4d\">4.2 Preparação para o Treinamento\n\n    Antes de iniciar o treinamento, deve-se configurar o arquivo YAML que descreverá os caminhos das imagens e anotações para cada conjunto de dados. Este arquivo serve como uma espécie de mapa para o YOLOv8, permitindo que o modelo saiba onde encontrar os dados necessários para o treinamento e a validação.","metadata":{}},{"cell_type":"code","source":"yaml_content = f\"\"\"\ntrain: /kaggle/working/train/images\nval: /kaggle/working/val/images\ntest: /kaggle/working/test/images\n\nnc: 1\nnames: ['cat']\n\"\"\"\n\nwith open('/kaggle/working/dataset.yaml', 'w') as yaml_file:\n    yaml_file.write(yaml_content)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:03.624772Z","iopub.execute_input":"2024-03-07T12:01:03.625021Z","iopub.status.idle":"2024-03-07T12:01:03.630265Z","shell.execute_reply.started":"2024-03-07T12:01:03.624999Z","shell.execute_reply":"2024-03-07T12:01:03.629338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"    O arquivo YAML necessário para o treinamento do modelo YOLOv8 foi criado. Ele contém os caminhos para os diretórios de treinamento, validação e teste, além da definição da classe que o modelo deve detectar.\n","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">5. Treinamento do Modelo YOLOv8\n\nApós configurar o ambiente e preparar os dados, iniciaremos o treinamento do modelo YOLOv8. Essa etapa é fundamental para aprender as características visuais dos gatos em diversas imagens e contextos. Neste momento, também nos certificamos de remover qualquer dependência desnecessária, como o `wandb`, que pode causar conflitos durante o treinamento, conforme mencionado na issue [#2073 do GitHub](https://github.com/ultralytics/ultralytics/issues/2073).\n","metadata":{}},{"cell_type":"code","source":"!pip3 uninstall -y wandb","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:03.631670Z","iopub.execute_input":"2024-03-07T12:01:03.632623Z","iopub.status.idle":"2024-03-07T12:01:07.135694Z","shell.execute_reply.started":"2024-03-07T12:01:03.632589Z","shell.execute_reply":"2024-03-07T12:01:07.134374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A desinstalação do `wandb` foi bem-sucedida, evitando os problemas conhecidos de incompatibilidade com a biblioteca Ultralytics durante o treinamento do modelo.","metadata":{}},{"cell_type":"markdown","source":"Prosseguimos para o treinamento do modelo utilizando a arquitetura YOLOv8. Nesse processo, definimos os parâmetros essenciais para o treinamento, incluindo o número de épocas, o tamanho do lote, a dimensão da imagem, a utilização de data augmentation, entre outros. Utilizamos um modelo pré-treinado YOLOv8m para acelerar a convergência e melhorar a precisão. O treinamento é feito com o objetivo de ajustar os pesos do modelo às particularidades do nosso conjunto de dados.","metadata":{}},{"cell_type":"code","source":"dataset_yaml = '/kaggle/working/dataset.yaml'\n\nmodel = YOLO('yolov8m.pt')\n\nresults = model.train(data=dataset_yaml, epochs=50, batch=16, imgsz=640, device=0, augment=True,verbose=True, cache=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:01:07.140762Z","iopub.execute_input":"2024-03-07T12:01:07.141094Z","iopub.status.idle":"2024-03-07T12:08:58.803745Z","shell.execute_reply.started":"2024-03-07T12:01:07.141062Z","shell.execute_reply":"2024-03-07T12:08:58.802393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os resultados do treinamento são salvos automaticamente pelo framework Ultralytics. Os pesos do modelo com melhor desempenho são armazenados para uso posterior em inferências. Este procedimento permite a reutilização eficiente do modelo treinado, sem a necessidade de repetir o treinamento.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">6. Avaliação do Modelo YOLOv8m nos Dados de Teste\n    \nAvaliamos a performance do modelo nos dados de teste para entender como ele se comporta com imagens que não foram usadas durante o treinamento. Isso nos dá uma medida objetiva de como o modelo pode performar no mundo real, onde enfrentará cenários variados e não vistos anteriormente.\n","metadata":{}},{"cell_type":"code","source":"model = YOLO('/kaggle/working/runs/detect/train/weights/best.pt')\n\ntest_results = model.val(split='test')","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:08:58.806217Z","iopub.execute_input":"2024-03-07T12:08:58.806596Z","iopub.status.idle":"2024-03-07T12:09:05.845981Z","shell.execute_reply.started":"2024-03-07T12:08:58.806535Z","shell.execute_reply":"2024-03-07T12:09:05.844770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os resultados da avaliação nos dão importantes métricas de desempenho, como precisão média e recall. Com essas informações, podemos entender melhor quais aspectos do modelo podem ser aprimorados e como ele pode ser ajustado para melhorar a detecção em casos desafiadores.","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#4d4d4d\">7. Inferências com o Modelo YOLOv8m\n    \nCom o modelo treinado e avaliado, agora realizaremos inferências em novas imagens para ver o modelo em ação. Esta etapa ilustra a aplicação prática do YOLOv8, permitindo visualizar como ele identifica e localiza gatos nas imagens, refletindo seu desempenho no mundo real.\n","metadata":{}},{"cell_type":"code","source":"results = model.predict('/kaggle/input/cats-dataset/cats_dataset/inference_imgs', conf=0.2, augment=True, iou=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:16:49.183670Z","iopub.execute_input":"2024-03-07T12:16:49.184462Z","iopub.status.idle":"2024-03-07T12:16:49.386344Z","shell.execute_reply.started":"2024-03-07T12:16:49.184430Z","shell.execute_reply":"2024-03-07T12:16:49.385376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for result in results:\n    result.save()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:16:51.019046Z","iopub.execute_input":"2024-03-07T12:16:51.019771Z","iopub.status.idle":"2024-03-07T12:16:51.068350Z","shell.execute_reply.started":"2024-03-07T12:16:51.019734Z","shell.execute_reply":"2024-03-07T12:16:51.067408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Com as inferências concluídas, salvamos os resultados que incluem as imagens com as bounding boxes e as classificações previstas pelo modelo. Estas imagens oferecem uma visão clara de como o modelo interpreta novas entradas e a precisão de suas detecções.\n","metadata":{}},{"cell_type":"code","source":"img_path1 = '/kaggle/working/results_405340C1-2E2D-4DA3-889F-14583CFFCEE8.JPG'\nimg_path2 = '/kaggle/working/results_E4F48723-FCE8-45F6-AC7E-4977A4E3EEEA.JPG'\n\nimg1 = Image.open(img_path1)\nimg2 = Image.open(img_path2)\n\nfig, axs = plt.subplots(2, 1, figsize=(6, 12))\n\naxs[0].imshow(img1)\naxs[0].axis('off') \n\naxs[1].imshow(img2)\naxs[1].axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-07T12:16:54.204611Z","iopub.execute_input":"2024-03-07T12:16:54.205319Z","iopub.status.idle":"2024-03-07T12:16:54.702594Z","shell.execute_reply.started":"2024-03-07T12:16:54.205287Z","shell.execute_reply":"2024-03-07T12:16:54.701525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Na última imagem inferida, o modelo identificou apenas a cabeça do gato. Isso destaca uma limitação interessante e uma área de potencial melhoria. Como o conjunto de treinamento não incluiu gatos com roupas, o modelo não aprendeu a reconhecer essa situação específica como uma única entidade. No entanto, o fato de identificar características distintas de gatos, como a cabeça, demonstra que o modelo aprendeu aspectos cruciais de sua forma geral e pode ser refinado com um conjunto de dados mais variado para melhorar ainda mais a precisão.","metadata":{}}]}